{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdwangco/SentimentAnalysisProject/blob/main/MLProjectYTSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7mzprkQCAfJZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from scipy.stats.stats import RanksumsResult\n",
        "# Evaluate the test using Flair baseline\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import transformers as ppb\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kDjB8YCbIFyn"
      },
      "outputs": [],
      "source": [
        "#input data source here\n",
        "df = pd.read_csv('data/train.tsv', delimiter='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "J1vbWnNTCZLM"
      },
      "outputs": [],
      "source": [
        "df.head()\n",
        "df[1].value_counts() #we have a balanced data set\n",
        "df = df[:2000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9P5Rvhb7EGrQ"
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def getVader(text):\n",
        "  score = analyzer.polarity_scores(text)\n",
        "  return 1 if score['compound'] >= 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yX-mN0uCFds4",
        "outputId": "5ab3c33f-ca4e-46c5-b37a-ad1e1f4447f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>Vader</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting room f...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  1  Vader\n",
              "0  a stirring , funny and finally transporting re...  1      1\n",
              "1  apparently reassembled from the cutting room f...  0      0\n",
              "2  they presume their audience wo n't sit still f...  0      1\n",
              "3  this is a visually stunning rumination on love...  1      1\n",
              "4  jonathan parker 's bartleby should have been t...  1      1"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Vader'] = df[0].apply(getVader)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhdsO5WBoiR2",
        "outputId": "c7e7843d-79a6-4e89-cb04-472f57a3f4a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-12-02 12:41:14,538 loading file /Users/josegarza/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        }
      ],
      "source": [
        "classifier = TextClassifier.load('en-sentiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "bhlim6En6UkU"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "    \"\"\" Predict the sentiment of a sentence \"\"\"\n",
        "    if sentence == \"\":\n",
        "        return 0\n",
        "    text = Sentence(sentence)\n",
        "    # stacked_embeddings.embed(text)\n",
        "    classifier.predict(text)\n",
        "    value = text.labels[0].to_dict()['value'] \n",
        "    return 0 if value == 'NEGATIVE' else 1\n",
        "\n",
        "def flairPredict(sentence):\n",
        "  result = predict(sentence)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJHbNBivkV2S",
        "outputId": "eddcb939-d29e-478f-d69c-7d65195e31db"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mFlairScore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(flairPredict)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(df))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb Cell 9\u001b[0m in \u001b[0;36mflairPredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflairPredict\u001b[39m(sentence):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   result \u001b[39m=\u001b[39m predict(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[1;32m/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb Cell 9\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m Sentence(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# stacked_embeddings.embed(text)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mpredict(text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m value \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlabels[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_dict()[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNEGATIVE\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/nn/model.py:705\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[0;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch:\n\u001b[1;32m    703\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m embedded_data_points, gold_labels, data_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_pass(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    706\u001b[0m     batch, for_prediction\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    708\u001b[0m \u001b[39m# if anything could possibly be predicted\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_points) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/models/text_classification_model.py:62\u001b[0m, in \u001b[0;36mTextClassifier.forward_pass\u001b[0;34m(self, sentences, for_prediction)\u001b[0m\n\u001b[1;32m     59\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[1;32m     61\u001b[0m \u001b[39m# embed sentences\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_embeddings\u001b[39m.\u001b[39;49membed(sentences)\n\u001b[1;32m     64\u001b[0m \u001b[39m# make tensor for all embedded sentences in batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m embedding_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embeddings\u001b[39m.\u001b[39mget_names()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/embeddings/base.py:62\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     59\u001b[0m     data_points \u001b[39m=\u001b[39m [data_points]\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_everything_embedded(data_points) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_embeddings:\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_internal(data_points)\n\u001b[1;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m data_points\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/embeddings/base.py:766\u001b[0m, in \u001b[0;36mTransformerEmbedding._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     expanded_sentences\u001b[39m.\u001b[39mextend(sentences)\n\u001b[0;32m--> 766\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_to_sentences(expanded_sentences)\n\u001b[1;32m    768\u001b[0m \u001b[39m# move embeddings from context back to original sentence (if using context)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_length \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/embeddings/base.py:692\u001b[0m, in \u001b[0;36mTransformerEmbedding._add_embeddings_to_sentences\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    689\u001b[0m gradient_context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39menable_grad() \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_tune \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    691\u001b[0m \u001b[39mwith\u001b[39;00m gradient_context:\n\u001b[0;32m--> 692\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[39m# make the tuple a tensor; makes working with it easier.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(hidden_states)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    568\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    569\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    570\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    574\u001b[0m )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:345\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    343\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 345\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    346\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:283\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    284\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    285\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    286\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    287\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    288\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    289\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    292\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:225\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(weights, v)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m context \u001b[39m=\u001b[39m unshape(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_lin(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m (context, weights)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "df['FlairScore'] = df[0].apply(flairPredict)\n",
        "df.head()\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTE0CQjKDhnB"
      },
      "outputs": [],
      "source": [
        "def getSubj(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "def getPol(text):\n",
        "  return TextBlob(text).sentiment.polarity\n",
        "\n",
        "def binarize(float):\n",
        "  return 1 if float > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "R4WFDQshPg7Q",
        "outputId": "157a636b-0b89-4480-ec8f-d159bdac6b17"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment</th>\n",
              "      <th>Label</th>\n",
              "      <th>Vader</th>\n",
              "      <th>FlairScore</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>TextBlob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting room f...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.275000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Comment  Label  Vader  \\\n",
              "0  a stirring , funny and finally transporting re...      1      1   \n",
              "1  apparently reassembled from the cutting room f...      0      0   \n",
              "2  they presume their audience wo n't sit still f...      0      1   \n",
              "3  this is a visually stunning rumination on love...      1      1   \n",
              "4  jonathan parker 's bartleby should have been t...      1      1   \n",
              "\n",
              "   FlairScore  Polarity  TextBlob  \n",
              "0           1  0.125000         1  \n",
              "1           0 -0.275000         0  \n",
              "2           0  0.178571         1  \n",
              "3           1  0.500000         1  \n",
              "4           0  0.200000         1  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "data = df.rename(columns={0: 'Comment', 1:'Label'})\n",
        "data['Polarity'] = data['Comment'].apply(getPol)\n",
        "data['TextBlob'] = data['Polarity'].apply(binarize)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLEnVUQuYtLO"
      },
      "outputs": [],
      "source": [
        "# Do BERT in parallel to compare baseline performance\n",
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-RTOQrjbJE1",
        "outputId": "cb05c8e5-4922-4cc4-b1f6-baedc6483cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
            "1    [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
            "2    [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
            "3    [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
            "4    [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
            "Name: Comment, dtype: object\n",
            "(2000, 59)\n",
            "(2000, 59)\n"
          ]
        }
      ],
      "source": [
        "tokenized = data['Comment'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "print(tokenized.head())\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "print(np.array(padded).shape)\n",
        "\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "print(attention_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhmagIMhkzxz"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "features = last_hidden_states[0][:,0,:].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 59]) and attention mask torch.Size([2000, 59])\n"
          ]
        }
      ],
      "source": [
        "print(input_ids.shape, 'and attention mask', attention_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmjSIgM8eXp9",
        "outputId": "35d8ffd4-70fb-4672-a63e-bcf22c7ae1ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9085\n"
          ]
        }
      ],
      "source": [
        "lr_clf = LogisticRegression()\n",
        "clf = lr_clf.fit(features,data['Label'])\n",
        "data['Bert'] = clf.predict(features)\n",
        "data.head()\n",
        "print(lr_clf.score(features, data['Label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nidb3peSM5EY",
        "outputId": "31f167b9-e581-48d4-9ce9-8b78982dd23f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob Confusion Matrix\n",
            "[[571 388]\n",
            " [292 749]]\n",
            "accuracy =  0.66\n",
            "Vader Confusion Matrix\n",
            "[[450 509]\n",
            " [154 887]]\n",
            "accuracy =  0.6685\n",
            "BERT Confusion Matrix\n",
            "[[880  79]\n",
            " [104 937]]\n",
            "accuracy =  0.9085\n",
            "Flair Confusion Matrix\n",
            "[[ 941   18]\n",
            " [  23 1018]]\n",
            "accuracy =  0.9795\n"
          ]
        }
      ],
      "source": [
        "print('TextBlob Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['TextBlob']))\n",
        "textBlobAcc = accuracy_score(data['Label'],data['TextBlob'])\n",
        "print('accuracy = ', textBlobAcc)\n",
        "\n",
        "print('Vader Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['Vader']))\n",
        "vaderAcc = accuracy_score(data['Label'],data['Vader'])\n",
        "print('accuracy = ',vaderAcc)\n",
        "\n",
        "print('BERT Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['Bert']))\n",
        "bertAcc = accuracy_score(data['Label'],data['Bert'])\n",
        "print('accuracy = ',bertAcc)\n",
        "\n",
        "print('Flair Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['FlairScore']))\n",
        "flairAcc = accuracy_score(data['Label'],data['FlairScore'])\n",
        "print('accuracy = ',flairAcc)\n",
        "\n",
        "names = ['TextBlob','Vader', 'BERT', 'Flair']\n",
        "accuracies = [textBlobAcc,vaderAcc, bertAcc, flairAcc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "pIdsihP0n_m4",
        "outputId": "b34071cd-025c-43e1-e83b-458a8a569582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.66, 0.6685, 0.9085, 0.9795]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 4 artists>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.title('Baseline Model Accuracy with No Training')\n",
        "print(accuracies)\n",
        "plt.bar(names,accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_to_text = {0:'BAD', 1:'GOOD'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GOOD: I love the way you explained, ...\n",
            "BAD: I hate the way you explained, ...\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I love the way you explained, plus the diversity :)\"\n",
        "max_len = 30\n",
        "sentence_pred = flairPredict(sentence)\n",
        "result = f'{score_to_text[sentence_pred]}: {sentence[:max_len]}...'\n",
        "print(result)\n",
        "\n",
        "sentence = \"I hate the way you explained, plus no diversity\"\n",
        "sentence_pred = flairPredict(sentence)\n",
        "result = f'{score_to_text[sentence_pred]}: {sentence[:max_len]}...'\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPbsO1gVjmxC2epR4DuWqiD",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "4945012c8ebc097ef9b1d0a7dea5b9b50f69c4e53b9d780ce4c29201fedc73a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
