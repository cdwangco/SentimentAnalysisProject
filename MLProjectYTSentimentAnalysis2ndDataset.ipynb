{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdwangco/SentimentAnalysisProject/blob/main/MLProjectYTSentimentAnalysis2ndDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7mzprkQCAfJZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from scipy.stats.stats import RanksumsResult\n",
        "# Evaluate the test using Flair baseline\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import transformers as ppb\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vzWl4jnGSTyG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/comments.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1vbWnNTCZLM",
        "outputId": "dc2a402a-a526-4b44-cf95-638966cf5877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0     Video ID                                            Comment  \\\n",
            "0           0  wAZZ-UWGVHI  Let's not forget that Apple Pay in 2014 requir...   \n",
            "1           1  wAZZ-UWGVHI  Here in NZ 50% of retailers don’t even have co...   \n",
            "2           2  wAZZ-UWGVHI  I will forever acknowledge this channel with t...   \n",
            "3           3  wAZZ-UWGVHI  Whenever I go to a place that doesn’t take App...   \n",
            "4           4  wAZZ-UWGVHI  Apple Pay is so convenient, secure, and easy t...   \n",
            "\n",
            "   Likes  Sentiment  \n",
            "0   95.0        1.0  \n",
            "1   19.0        0.0  \n",
            "2  161.0        2.0  \n",
            "3    8.0        0.0  \n",
            "4   34.0        2.0  \n",
            "2.0    1096\n",
            "1.0     556\n",
            "0.0     348\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n",
        "df = df[:2000]\n",
        "print(df['Sentiment'].value_counts()) #we have a balanced data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "nKR8eZ3lTfk2"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[:,[2,4]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JkmWurreVn_y",
        "outputId": "3d0debe0-6364-4c98-bdbe-632d5db8aca4"
      },
      "outputs": [],
      "source": [
        "df['1'] = df['Sentiment'].apply(lambda x: 0 if x <= 1 else 1)\n",
        "df = df.iloc[:,[0,2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "P0usDrenWhzM",
        "outputId": "0fba3037-27ef-461e-b794-f1456c56eb57"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'Comment': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9P5Rvhb7EGrQ"
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def getVader(text):\n",
        "  score = analyzer.polarity_scores(text)\n",
        "  return 1 if score['compound'] >= 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yX-mN0uCFds4",
        "outputId": "187e544e-0210-4027-9ea4-c33851768b3e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>Vader</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Let's not forget that Apple Pay in 2014 requir...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Here in NZ 50% of retailers don’t even have co...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I will forever acknowledge this channel with t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Whenever I go to a place that doesn’t take App...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Apple Pay is so convenient, secure, and easy t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  1  Vader\n",
              "0  Let's not forget that Apple Pay in 2014 requir...  0      1\n",
              "1  Here in NZ 50% of retailers don’t even have co...  0      1\n",
              "2  I will forever acknowledge this channel with t...  1      1\n",
              "3  Whenever I go to a place that doesn’t take App...  0      0\n",
              "4  Apple Pay is so convenient, secure, and easy t...  1      1"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Vader'] = df[0].apply(getVader)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhdsO5WBoiR2",
        "outputId": "b0ca0c65-b1c0-4e60-f823-d47824be6d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-12-02 12:28:14,634 loading file /Users/josegarza/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        }
      ],
      "source": [
        "classifier = TextClassifier.load('en-sentiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bhlim6En6UkU"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "    \"\"\" Predict the sentiment of a sentence \"\"\"\n",
        "    if sentence == \"\":\n",
        "        return 0\n",
        "    text = Sentence(sentence)\n",
        "    # stacked_embeddings.embed(text)\n",
        "    classifier.predict(text)\n",
        "    value = text.labels[0].to_dict()['value']\n",
        "    return 0 if value == 'NEGATIVE' else 1\n",
        "\n",
        "def flairPredict(sentence):\n",
        "  result = predict(sentence)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJHbNBivkV2S",
        "outputId": "807edf94-cd44-40b3-b1e4-b18b209a6126"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mFlairScore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(flairPredict)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(df))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb Cell 12\u001b[0m in \u001b[0;36mflairPredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflairPredict\u001b[39m(sentence):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   result \u001b[39m=\u001b[39m predict(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[1;32m/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb Cell 12\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m Sentence(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# stacked_embeddings.embed(text)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mpredict(text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m value \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlabels[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_dict()[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josegarza/Desktop/SentimentAnalysisProject/MLProjectYTSentimentAnalysis2ndDataset.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNEGATIVE\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/nn/model.py:705\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[0;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch:\n\u001b[1;32m    703\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m embedded_data_points, gold_labels, data_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_pass(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    706\u001b[0m     batch, for_prediction\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    708\u001b[0m \u001b[39m# if anything could possibly be predicted\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_points) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/models/text_classification_model.py:62\u001b[0m, in \u001b[0;36mTextClassifier.forward_pass\u001b[0;34m(self, sentences, for_prediction)\u001b[0m\n\u001b[1;32m     59\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[1;32m     61\u001b[0m \u001b[39m# embed sentences\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_embeddings\u001b[39m.\u001b[39;49membed(sentences)\n\u001b[1;32m     64\u001b[0m \u001b[39m# make tensor for all embedded sentences in batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m embedding_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embeddings\u001b[39m.\u001b[39mget_names()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/embeddings/base.py:62\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     59\u001b[0m     data_points \u001b[39m=\u001b[39m [data_points]\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_everything_embedded(data_points) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_embeddings:\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_internal(data_points)\n\u001b[1;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m data_points\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/embeddings/base.py:766\u001b[0m, in \u001b[0;36mTransformerEmbedding._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     expanded_sentences\u001b[39m.\u001b[39mextend(sentences)\n\u001b[0;32m--> 766\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_to_sentences(expanded_sentences)\n\u001b[1;32m    768\u001b[0m \u001b[39m# move embeddings from context back to original sentence (if using context)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_length \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/flair/embeddings/base.py:692\u001b[0m, in \u001b[0;36mTransformerEmbedding._add_embeddings_to_sentences\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    689\u001b[0m gradient_context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39menable_grad() \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_tune \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    691\u001b[0m \u001b[39mwith\u001b[39;00m gradient_context:\n\u001b[0;32m--> 692\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[39m# make the tuple a tensor; makes working with it easier.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(hidden_states)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    568\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    569\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    570\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    574\u001b[0m )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:345\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    343\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 345\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    346\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:299\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    296\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa_layer_norm(sa_output \u001b[39m+\u001b[39m x)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn(sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    300\u001b[0m ffn_output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer_norm(ffn_output \u001b[39m+\u001b[39m sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    302\u001b[0m output \u001b[39m=\u001b[39m (ffn_output,)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:244\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_chunking_to_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, \u001b[39minput\u001b[39;49m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:247\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mff_chunk\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 247\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin1(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    248\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m    249\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin2(x)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "df['FlairScore'] = df[0].apply(flairPredict)\n",
        "df.head()\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTE0CQjKDhnB"
      },
      "outputs": [],
      "source": [
        "def getSubj(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "def getPol(text):\n",
        "  return TextBlob(text).sentiment.polarity\n",
        "\n",
        "def binarize(float):\n",
        "  return 1 if float > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "R4WFDQshPg7Q",
        "outputId": "ef8e7dee-9ca6-4a37-d9f2-755dfa82e7d6"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = df.rename(columns={0: 'Comment', '1':'Label'})\n",
        "data['Polarity'] = data['Comment'].apply(getPol)\n",
        "data['TextBlob'] = data['Polarity'].apply(binarize)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLEnVUQuYtLO"
      },
      "outputs": [],
      "source": [
        "# Do BERT in parallel to compare baseline performance\n",
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['Comment'] = [d[:512] for d in data['Comment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-RTOQrjbJE1",
        "outputId": "a42c675e-e5b0-47e8-f2b6-f07343f69a70"
      },
      "outputs": [],
      "source": [
        "tokenized = data['Comment'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "print(tokenized.head())\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "print(f'max length: {max_len}')\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "print(np.array(padded).shape)\n",
        "\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "print(attention_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhmagIMhkzxz",
        "outputId": "be9b180c-8a5b-413a-dca6-a951e510c2d6"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "print(input_ids.shape, 'and attention mask', attention_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "    print(last_hidden_states)\n",
        "features = last_hidden_states[0][:,0,:].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmjSIgM8eXp9"
      },
      "outputs": [],
      "source": [
        "lr_clf = LogisticRegression()\n",
        "clf = lr_clf.fit(features,data['Label'])\n",
        "data['Bert'] = clf.predict(features)\n",
        "data.head()\n",
        "print(lr_clf.score(features, data['Label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nidb3peSM5EY"
      },
      "outputs": [],
      "source": [
        "print('TextBlob Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['TextBlob']))\n",
        "textBlobAcc = accuracy_score(data['Label'],data['TextBlob'])\n",
        "print('accuracy = ', textBlobAcc)\n",
        "\n",
        "print('Vader Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['Vader']))\n",
        "vaderAcc = accuracy_score(data['Label'],data['Vader'])\n",
        "print('accuracy = ',vaderAcc)\n",
        "\n",
        "print('BERT Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['Bert']))\n",
        "bertAcc = accuracy_score(data['Label'],data['Bert'])\n",
        "print('accuracy = ',bertAcc)\n",
        "\n",
        "print('Flair Confusion Matrix')\n",
        "print(confusion_matrix(data['Label'],data['FlairScore']))\n",
        "flairAcc = accuracy_score(data['Label'],data['FlairScore'])\n",
        "print('accuracy = ',flairAcc)\n",
        "\n",
        "names = ['TextBlob','Vader', 'BERT', 'Flair']\n",
        "accuracies = [textBlobAcc,vaderAcc, bertAcc, flairAcc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIdsihP0n_m4"
      },
      "outputs": [],
      "source": [
        "plt.title('Baseline Model Accuracy with No Training')\n",
        "print(accuracies)\n",
        "plt.bar(names,accuracies)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMSEXVsOJvPQpVxejqCp15Q",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
